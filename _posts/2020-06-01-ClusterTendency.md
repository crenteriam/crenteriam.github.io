<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true},
jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
TeX: {
extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
equationNumbers: {
autoNumber: "AMS"
}
}
});
</script>

Here, I share the code and notes from the cluster validation procedures
that I did for my dissertation. By construction, both the hierarchical
and partitioning clustering techniques will produce outputs with as many
clusters as the researcher defines, even though not all of them are
meaningful. Therefore, two of the main challenges in cluster analysis
are identifying whether the data has a cluster structure (i.e. it is not
a randomly drawn distribution of cases), and identifying a range of
possible relevant and meaningful clusters within the data. In the next
section, I present an assessment of cluster tendency, which provides
statistical insights about whether the data set contains a cluster
structure. Next, I present informative statistics to assess the possible
number of relevant clusters existing in the data. Finally, I present the
results of the Silhouette Analysis, which show the goodness of the
number of clusters defined in the study.

I begin by sharing the data and the setup evironment. The data sets are
located in:

``` r
# File URL location
# XXXXX

# Environment Setup ------------------------------------------------------------------
rm(list=ls()) # Clear Workspace
options(stringsAsFactors = TRUE, digits=2)
wd.dir      = "SET YOUR OWN PATH DIRECTORY HERE"
wd.data    = paste0(wd.dir, "/3_data/")
wd.displays=  paste0(wd.dir, "/4_displays/")
setwd(wd.dir)

# Packages
# cluster - custering algorithms; factoextra - clustering and visualization; ggdendro - plotting dendograms
# factoextra for data visualization; clustertend for statistical assessment clustering tendency.
# NbClust for computing about 30 methods at once, in order to find the optimal number of clusters.
# fpc for computing clustering validation statistics
# pvclust uses bootstrap resampling techniques to compute p-value for each hierarchical clusters
Packages <- c("tidyverse", "cluster", "factoextra", "clustertend", "NbClust", "fpc", "pvclust", "ggpubr", "readxl", "ggdendro", "xtable")
#install.packages(Packages,
#                 lib = "S:/packages")
lapply(Packages, library, character.only = TRUE)
```

Analysis of cluster Tendency
----------------------------

I used two techniques of assessment of cluster tendency to verify
whether the data structure contains meaningful and relevant clusters:
the Hopkins Statistics and the Visual Assessment of cluster Tendency
(VAT). The Hopkins Statistic assesses the probability that a given data
is generated by a random process (i.e. a non-structured process). The
Hopkins statistic estimation begins by first sampling *n* points from
the data set. Afterwards, for each observation (*p*<sub>*i*</sub>) a
nearest neighbor (*p*<sub>*j*</sub>) is found and the distance is
computed (\\(x\_i = dist(p\_i,p\_j)\\)). Then, a second data set is
created through a simulation yielding a random distribution
(i.e. without a pattern or structure); then, the same process is
computed
(*y*<sub>*i*</sub> = *d**i**s**t*(*q*<sub>*i*</sub>, *q*<sub>*j*</sub>)).
The Hopkins statistic (H) is obtained by dividing the sum of nearest
neighbors from *y*<sub>*i*</sub> by the combined sum of nearest
neighbors *x*<sub>*i*</sub> + *y*<sub>*i*</sub>. An H value close to 0.5
indicates that $\\sum\_{i=1}^{n}x\_i$ is close to a random distribution,
and therefore there is not a cluster structure within the data. An H
value close to 0 or 1 indicates a meaningful and relevant cluster
structure within the data.

$ \\begin{equation} H = $

``` r
# Compute Hopkins statistic with n = N-1
hopkins(data.participation.short, n = nrow(data.crowd)-1)
```

After conducting the equation shown above in the data set containing all
the crowdsourcing initiatives, the resulting Hopkins Statistic was 0.36,
indicating a weak cluster structure within the data.

The VAT analysis uses the dissimilarity matrix between the observations
from the data set and sorted by minimal distance. The resulting data,
known as Ordered Dissimilarity Matrix (ODM) is displayed through a heat
map. If the data contains a cluster structure, then the visual
representation of the ODM must have a discernible pattern.

The Figure below shows a pattern in the ODM, where the top side and left
side show a cluster of blue values (i.e. high dissimilarity), while the
rest of the Figure shows a cluster of red values (i.e. small
dissimilarity).

``` r
# Visual Assessment of cluster Tendency (VAT)
fviz_dist(dist(data.participation.short), show_labels = FALSE) + labs(title = "Crowdsourcing Initiatives Dataset")
ggsave("Cluster_VAT.png", path = wd.displays, height = 10, width = 10*aspect_ratio, units = "cm")
```

Estimation of Number of Clusters
--------------------------------

Once, there is evidence that the data set contains a cluster structure,
it is useful to estimate statistics indicating the number of clusters
that the data might have. Identifying the number of clusters, however,
is not an objective or statistical procedure; the best approach is a
blend of statistical insights and researchers’ expertise in the topic
and a careful qualitative appraisal of the data.

To assist my decision in defining the of clusters in the analysis, I
used the Elbow and the Silhouette methods. The Elbow method identifies
the number of clusters that minimize the total within-cluster sum of
squares (WSS). Every additional cluster will decrease the total WSS,
therefore, the criterion for selecting an optimal number of clusters is
keep adding clusters until the next cluster does not substantially
decreases the total WSS. The Figure below shows that the total WSS
decreases substantially until the fourth cluster; then, the slope
between the fourth and the fifth clusters—and the subsequent
slopes—becomes less sharp. Following the Elbow method’s statistics, the
optimal number of clusters is four.

FIGURE HERE includegraphics{Displays/Cluster\_Elbow.png} caption{Elbow
Method to Estimate an Optimal Number of Clusters}

The Average Silhouette method measures the width of the cluster’s
silhouette. In this technique, the criterion for determining the optimal
number of clusters the maximization of the maximization of the average
silhouette. The Figure below indicates that, following the Silhouette
method, the optimal number of clusters is two.

FIGURE includegraphics{Displays/Cluster\_Silhouette.png}
caption{Silhouette Method to Estimate an Optimal Number of Clusters}

Combined, the Elbow and Average Silhouette techniques suggest that the
relevant and meaningful clusters that the data might have may be between
two and four clusters.

``` r
# Optimal Number of Clusters  ------------------------------------------
# fviz_nbclust() function [in factoextra R package]: It can be used to compute the three different methods [elbow, silhouette and gap statistic] for any partitioning clustering methods [K-means, K-medoids (PAM), CLARA, HCUT].
# NbClust() function [in NbClust R package] (Charrad et al., 2014): It provides 30 indices for determining the relevant number of clusters

# Elbow method
fviz_nbclust(data.participation.short, kmeans, method = "wss") + geom_vline(xintercept = 4, linetype = 2) + labs(subtitle = "Elbow method")
ggsave("Cluster_Elbow.png", path = wd.displays, height = 10, width = 10*aspect_ratio, units = "cm")

# Silhouette method
fviz_nbclust(data.participation.short, kmeans, method = "silhouette") + labs(subtitle = "Silhouette method")
ggsave("Cluster_Silhouette.png", path = wd.displays, height = 10, width = 10*aspect_ratio, units = "cm")

# Gap statistic
# recommended value: nboot= 500 for your analysis. Use verbose = FALSE to hide computing progression.
fviz_nbclust(data.participation.short, kmeans, nstart = 25, method = "gap_stat", nboot = 500) + labs(subtitle = "Gap statistic method")
ggsave("Cluster_Gap.png", path = wd.displays, height = 10, width = 10*aspect_ratio, units = "cm")
```

Cluster Validation Statistics
-----------------------------

To assess the goodness of clustering results is is important to perform
an internal cluster validation of the clustering algorithms. Good
clustering results must reflect compactness (the closeness among the
observations within the same cluster), connectivity (whether the
observations are placed in the same cluster as their nearest neighbor),
and separation (the separation among clusters).

One of the most widely used clustering validation techniques is the
Silhouette Analysis. The Silhouette Analysis estimates for each
observation (i.e. each crowdsourcing initiative), first, the average
dissimilarity between the observation and the rest of initiatives within
the same cluster, represented the parameter *a*<sub>*i*</sub> (see
Equation below). The resulting value in *a*<sub>*i*</sub> is compared
with the average dissimilarity between the observation and all the
observations contained in the nearest neighbor cluster—represented by
the parameter *b*<sub>*i*</sub>. A resulting value close to 1 means that
the observation is highly similar with the rest of the cluster, whereas
a resulting value of -1 reflects a poor similarity with the cluster. The
result of the calculation is an average Silhouette width of 0.22, which
is closer to 1 than to -1 and therefore indicates a good fit of every
observation to their cluster.

\\\[ S\_i = \\\]

``` r
# Silhouette Statistics
sd_participation_goals <- scale(data.participation.short) # Standardize
# eclust() (enhanced clustering). Simplifies the workflow of clustering analysis
cluster_partitioned.sd_participation <- eclust(sd_participation_goals, "kmeans", k = 6, nstart = 25, graph = FALSE) # K-means clustering
# fviz_cluster(cluster_partitioned.sd_participation, geom = "point", ellipse.type = "norm", palette = "jco", ggtheme = theme_minimal()) # Visualize k-means clusters

cluster_hierarchical.sd_participation <- eclust(sd_participation_goals, "hclust", k = 6, hc_metric = "euclidean", hc_method = "ward.D2", graph = FALSE) # Hierarchical clustering
fviz_dend(cluster_hierarchical.sd_participation, show_labels = FALSE, palette = "jco", as.ggplot = TRUE) # Visualize dendrograms


# Silhouette Information
silinfo <- cluster_partitioned.sd_participation$silinfo
names(silinfo) # Silhouette information
head(silinfo$widths[, 1:3], 100) # Silhouette widths of each observation (first 100 obs)
silinfo$clus.avg.widths # Average silhouette width of each cluster
silinfo$avg.width # The total average (mean of all individual silhouette widths)
cluster_partitioned.sd_participation$size # The size of each clusters

# Silhouette Plot
fviz_silhouette(cluster_partitioned.sd_participation, palette = "jco", ggtheme = theme_classic()) # Silhouette Plot
ggsave("Cluster_SilhouettePlot.png", path = wd.displays, height = 10, width = 10*aspect_ratio, units = "cm")
sil <- cluster_partitioned.sd_participation$silinfo$widths[, 1:3] # Silhouette width of observation
neg_sil_index <- which(sil[, 'sil_width'] < 0)  # Objects with negative silhouette
sil[neg_sil_index, , drop = FALSE]
```
